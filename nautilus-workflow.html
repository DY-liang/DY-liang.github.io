<head>
    <title>Nicklas Hansen</title>
    <meta property="og:title" content="Nicklas Hansen">
	<meta property="og:description" content="PhD student, UC San Diego">
    <meta property="og:image" content="https://nicklashansen.github.io/files/me.jpeg">
	<meta property="og:url" content="https://nicklashansen.github.io/">
	<meta name="twitter:card" content="summary_large_image">
    <link rel="apple-touch-icon" sizes="180x180" href="files/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="files/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="files/favicon-16x16.png">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
    <style>
        .center-img {display: block; margin: auto;}
        code {font-family: Consolas,"courier new"; color: crimson; background-color: #f1f1f1; padding: 2px; font-size: 105%;}
        .bullets li {margin-bottom: 12px;}
    </style>
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Nicklas Hansen</h1>
            </div>
            <div class="header-subtitle">
                PhD student, UC San Diego
            </div>
            <div class="header-links">
                <a class="btn" href="https://github.com/nicklashansen">GitHub</a>
                <a class="btn" href="https://twitter.com/ncklashansen">Twitter</a>
                <a class="btn" href="https://www.linkedin.com/in/ncklas/">LinkedIn</a>
                <a class="btn" href="https://scholar.google.com/citations?hl=en&user=8wGH7wsAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>
                <a class="btn" href="files/resume.pdf">Resume</a>
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <h1 class="noselect">Workflow Overview</h1>
        <h4 class="noselect italic">September 2021</h4>
        <br/>
        <p>
            Hi there! This is a brief introduction to my workflow when developing new projects and running experiments in Nautilus. As a disclaimer: <span class="italic">just because I do it this way does not mean it's the best way to do it</span>, and it is more than likely that I will do things differently in the future.
        </p>
        <p>
            I work on Reinforcement Learning (RL), which typically requires computationally intensive simulations and no significant reliance on external datasets. Experiments are rather short (typically between 12h and 2 days), but a large number of experiments are required in order to produce meaningful results. For example, reproducing the main results of our paper <a href="https://nicklashansen.github.io/SVEA/">Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation</a> requires running experiments on 5 different tasks from the DeepMind Control Suite, each with 6 different choices of data augmentation and each experiment is repeated 5 times to ensure that the results are reliable. With 150 experiments to run and an average duration of 24 hours, this translates to approximately 3,600 GPU hours excluding baselines and ablations. So running experiments efficiently and in parallel increases productivity dramatically.
        </p>
    </div>
    <div>
        <h2 class="noselect">Development Overview</h2>
        <p>
            My workflow is roughly as follows: I <code>ssh</code> into a remote machine through <a href="https://code.visualstudio.com/">Visual Studio Code</a>, and keep everything related to my project there, both source code, datasets, logs, figures, etc., and I use <code>git</code> to track changes. All of my development takes place on the remote machine, and results are viewed directly in Visual Studio Code. Experiments are run in Nautilus through <code>ssh</code> from my local machine, and all logs are stored on the same remote machine used for development (this transfer is done using <code>rsync</code>). With a decent directory structure for logs, it is easy to locate logs from specific experiments, and I have automated both plotting of aggregate results and keeping track of which experiments are unfinished using Python.
        </p>
        <img src="nautilus/development-overview.png" alt="Development Overview" class="center-img" width="30%">
    </div>
    <div>
        <h2 class="noselect">Managing Nautilus</h2>
        <p>
            I use a number of <code>bash</code> scripts and <code>alias</code>es to manage Nautilus jobs. While not everything is worth mentioning, I highlight a few of them here:<br/><br/>
            <ul class="bullets">
                <li>
                    Include <code>kubectl</code> completions: append <code>source <(kubectl completion bash)</code> to your <code>.bashrc</code>
                </li>
                <li>
                    Listing all namespaces: <code>alias context="kubectl config get-contexts"</code>
                </li>
                <li>
                    Short-hands for changing to individual namespaces: e.g. <code>alias multi="kubectl config use-context rl-multitask"</code>
                </li>
                <li>
                    Listing all of <span class="bold">my own</span> jobs and where they run, sorted by recency: <code>pods() {kubectl get pods --no-headers -o wide --sort-by={metadata.creationTimestamp} | grep 'nh-' | tac;}</code><br/>
                    In practice, I iterate over each namespace and account that I am actively using.
                </li>
                <li>
                    Deleting all completed jobs: <code>kubectl get pods --no-headers | grep 'nh-' | grep 'Completed' | cut -d ' ' -f1 | gawk '{print substr($1, 1, length($1)-6);}' | parallel -r -P 16 kubectl delete jobs</code><br/>
                    In practice, I use a <code>bash</code> script that iterates over all pod statuses that indicate completion or error.
                </li>
            </ul>
        </p>
    </div>
    <div>
        <h2 class="noselect">Submitting Jobs in Nautilus</h2>
        <p>
            For job submission, I have developed a small Python package that I call <code>nsub</code>, which interacts with Nautilus using the <code>kubernetes</code> Python API. The figure below gives an overview over its components that I explain in the following.
        </p>
        <img src="nautilus/nsub-overview.png" alt="nsub Overview" class="center-img" width="30%">
        <p>
            It requires a <code>job.yaml</code> configuration file describing which image to use, which scripts to run prior to the job itself, which resources to request, etc.; it is essentially a simplified version of the example from the <a href="https://ucsd-prp.gitlab.io/userdocs/tutorial/scheduling/">Nautilus documentation</a>. The <code>setup</code> field is necessary as there are certain files that I do not wish to keep in the Docker image itself, e.g. secrets and MuJoCo licenses, and I commonly use variants of packages that need to be dynamically linked as both their source code and locations change frequently. I typically do not modify the <code>job.yaml</code> file over the course of a project's lifetime. An example of the file is shown below:
        </p>
        <p>
            <code>
kind: Job<br/>
namespace: rl-multitask<br/>
image: nicklashansen/imrl:latest<br/>
setup: 'cd $VOLUME && cp secrets/* /root/.ssh/ && cp secrets/mjkey.txt /root/.mujoco/ &&<br/>
&emsp;cd $PROJECT && /bin/bash setup/install_envs.sh && export MUJOCO_GL=egl'<br/>
script: job.yaml<br/>
project: newdmc<br/>
resources:<br/>
&emsp;requests:<br/>
&emsp;&emsp;cpu: 2<br/>
&emsp;&emsp;memory: 18<br/>
&emsp;&emsp;gpu: 1<br/>
&emsp;limits:<br/>
&emsp;&emsp;cpu: 4<br/>
&emsp;&emsp;memory: 18<br/>
&emsp;&emsp;gpu: 1<br/>
&emsp;ephemeral_storage: 0<br/>
volume: svea<br/>
ephemeral_storage:<br/>
&emsp;source: data<br/>
&emsp;destination: /scratch
            </code>
        </p>
        <p>
            The <code>nsub</code> package has a function <code>submit_jobs</code> that take as argument a job name, a <code>bash</code> command to run, a <code>job.yaml</code> configuration file, as well as a list of seeds to run for that experiment. The function then creates the necessary job objects through the <code>kubernetes</code> API and submits them to Nautilus.
        </p>
        <p>
            To make life easier when running a large number of experiments, I additionally use a <code>job.py</code> script that details which experiments to run. To stick with the running example of our SVEA paper, this script might iterate over all 5 tasks from the DeepMind Control Suite, run our method with each of 6 data augmentations, and repeat each experiment with 5 different seeds (typically 0-4). Concretely, this file may look as follows:
        </p>
        <p>
            <code>
import nsub<br/>
import os<br/>
import random<br/>
import string<br/>
<br/>
algorithm = 'svea'<br/>
tasks = [<br/>
&emsp;('walker', 'walk', 4),<br/>
&emsp;('walker', 'stand', 4),<br/>
&emsp;('cartpole', 'swingup', 8),<br/>
&emsp;('ball_in_cup', 'catch', 4),<br/>
&emsp;('finger', 'spin', 2)<br/>
]<br/>
augs = [<br/>
&emsp;'conv',<br/>
&emsp;'overlay',<br/>
&emsp;'cutout',<br/>
&emsp;'rotation',<br/>
&emsp;'blur',<br/>
&emsp;'affine-jitter'<br/>
]<br/>
num_seeds = 5<br/>
<br/>
cmd = nsub.load_script(os.path.join(os.getcwd(), 'job.sh'))<br/>
<br/>
num_jobs = 0<br/>
uid = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))<br/>
for task in tasks:<br/>
&emsp;for aug in augs:<br/>
&emsp;&emsp;domain_name, task_name, action_repeat = task<br/>
&emsp;&emsp;job_name = f'{algorithm}-{task_name}-{aug}-{uid}-$ARRAY'<br/>
&emsp;&emsp;_cmd = cmd \<br/>
&emsp;&emsp;&emsp;.replace('$ALGORITHM', algorithm) \<br/>
&emsp;&emsp;&emsp;.replace('$DOMAIN', domain_name) \<br/>
&emsp;&emsp;&emsp;.replace('$TASK', task_name) \<br/>
&emsp;&emsp;&emsp;.replace('$ACTIONREPEAT', str(action_repeat)) \<br/>
&emsp;&emsp;&emsp;.replace('$MODE', aug) \<br/>
&emsp;&emsp;&emsp;.replace('$EXPSUFFIX', aug)<br/>
&emsp;&emsp;nsub.submit_jobs(_cmd, job_name, range(num_seeds))<br/>
&emsp;&emsp;num_jobs += num_seeds<br/>
<br/>
print(f'Submitted {num_jobs} job(s).')
            </code>
        </p>
        <p>
        and running this script has now submitted a total of 150 jobs in less than a second. Obviously, we probably shouldn't occupy this many GPUs at once, but it proves my point. The arguments shown in this script directly overwrite arguments in a <code>job.sh</code> file:
        </p>
        <p>
            <code>
python3 src/train.py \<br/>
&emsp;--domain_name $DOMAIN \<br/>
&emsp;--task_name $TASK \<br/>
&emsp;--action_repeat $ACTIONREPEAT \<br/>
&emsp;--algorithm $ALGORITHM \<br/>
&emsp;--aug $AUG \<br/>
&emsp;--save_video \<br/>
&emsp;--video_freq 100k \<br/>
&emsp;--save_freq 500k \<br/>
&emsp;--exp_suffix $EXPSUFFIX \<br/>
&emsp;--seed $ARRAY
            </code>
        </p>
        <p>
            although this is not strictly necessary. If you want access to my <code>nsub</code> package, feel free to reach out on Slack.
        </p>
    </div>
    <div>
        <h2 class="noselect">Experiment Tracking</h2>
        <p>
            This is (especially) where I need your help. With so many jobs running, I need to keep track of which jobs are still running, which jobs have completed, and which jobs have failed. Further, I need good tools for experiment logging, piping and aggregation of results, which I believe I currently don't have. My current setup is as follows. I use a log directory structure that looks like this:
        </p>
        <p>
            <code>
                - logs<br/>
                |  - walker_walk<br/>
                |  |  - svea<br/>
                |  |  |  - overlay<br/>
                |  |  |  |  - 0<br/>
                |  |  |  |  |  model/<br/>
                |  |  |  |  |  video/<br/>
                |  |  |  |  |  eval.log<br/>
                |  |  |  |  |  info.log<br/>
                |  |  |  |  |  train.log<br/>
            </code>
        </p>
        <p>
            where <code>info.log</code> contains time stamp, <code>git</code> hash, and the full list of arguments used in the experiment. <code>train.log</code> is a <code>yaml</code> file with training statistics in the following format:
        </p>
        <p>
            <code>
                {"episode_reward": 221.83129155216804, "episode": 40.0, "critic_loss": 12.678056478500366, "actor_loss": -32.06281546020508, "alpha_loss": 0.2850281069278717, "alpha_value": 0.06374894149871128, "aux_loss": 0.002858409071341157, "duration": 45.27503752708435, "step": 10000}
            </code>
        </p>
        <p>
            and <code>eval.log</code> contains the equivalent statistics for evaluation (if applicable). From the <code>train.log</code> and <code>eval.log</code> files, I aggregate all results into a single figure, and the plotting script automatically warns me if any results are missing, either partially (i.e. it either failed or is currently running) or entirely (i.e. it never ran). Using the example from previously, we get the following figure:
        </p>
        <img src="nautilus/stability.png" alt="nsub Overview" class="center-img" width="70%"><br/>
        <p>
            where we can optionally choose to plot standard deviations as well. While this approach works well for preparing results for publication, it is relatively cumbersome to work with during the early stages of a project, and I currently have no way to tell whether a job <span class="italic">failed</span> or is <span class="italic">currently</span> running.
        </p>
    </div>
    <div>
        <h2 class="noselect">GitHub Co-Pilot</h2>
        <p>
            Let me say this very clearly: <span class="bold">if you are not using Co-Pilot, you should <a href="https://copilot.github.com/" class="bold">sign up</a> immediately</span>. I would estimate that Co-Pilot writes about <span class="bold">20%</span> of my code, and about <span class="bold">50%</span> of my comments. Even some of this guide is written by Co-Pilot. It is very useful for everyday programming tasks using common libraries, and it can often do well in line completion and next-line prediction, but it is less useful for writing entire functions. Recognize when Co-Pilot is helpful and when it is not and it can increase your productivity substantially. To help convince you, here are some examples of Co-Pilot suggestions from my coding session today:
        </p>
        <p><span class="bold">Assertions:</span> Co-Pilot understands the meaning of the variables and suggests useful assertions, including error messages. In this example, all of the comments on the arguments are written by Co-Pilot as well, with only minor modifications to provide more detail (e.g. that we use Intel RealSense cameras).</p>
        <img src="nautilus/copilot0.png" alt="nsub Overview" class="center-img" width="70%"><br/>
        <p><span class="bold">Common library calls:</span> Knowing that I imported OpenCV in the top of the file, Co-Pilot understands the context of my if-statement and suggests to use OpenCV to save <code>obs</code> in the <code>self.observation_save_path</code> directory using the time step <code>self.t</code> as a formatted string. I rarely need to look up documentation for libraries anymore.</p>
        <img src="nautilus/copilot1.png" alt="nsub Overview" class="center-img" width="60%"><br/>
        <p><span class="bold">Function completion:</span> Co-Pilot understands that I am using <code>ROS</code> for processing image observations from a camera, and suggests a full <code>unsubscribe</code> function (including comments) based only the function name. Better yet, it even recognizes that all my camera subscriptions are stored in <code>self.cameras</code>.</p>
        <img src="nautilus/copilot2.png" alt="nsub Overview" class="center-img" width="35%"><br/>
        <p>Don't put blind trust in Co-Pilot, but sure do take advantage of it and save yourself from some typing.</p>
    </div>
    <div>
</div>